#!/home/btang/install/bin/python25

'''
execfile('/home/bytang/bin/shtpy/shtpy185a')

#execfile('/home/svc/new_github/CMDA/JPL_CMDA/services/svc/svc/src/updateData/shtpy185a')

kk sp
rsync8022 shtpy185a $cmda0:/home/bytang/bin/shtpy

rsync8022 shtpy185a $cmda0:/home/svc/new_github/CMDA/JPL_CMDA/services/svc/svc/src/updateData

rsyncec2 shtpy185a $ec21:/home/ubuntu/CMDA0/svc/src/updateData 


rsync8022 $ec21:/home/svc/new_github/CMDA/JPL_CMDA/services/svc/svc/src/updateData/shtpy185a  .

#rsync8022 shtpy185a $cmdad:bin/shtpy
rsync8022 shtpy185a $cmda1:bin/shtpy

'''
codeNum = '185a'
import os
#HOME=os.environ['HOME']
home='/home/bytang'   # for cmda4
HOME = home

cr = {
#                              1  2   3  4
'import_':              1,   # 0  1   1  0
# readme__
#parameters_':  
#parameters__':
#                              1  2  3  4
# 1 -- gather list of files
# 2 -- collect facets 
# 3 -- save js for web
'command_arg':    0, #  1,   # 1  1  1
'collect_fileList':     0,   # 1  0  0
'combine_break_files':  0,   # 1  0  0
'save_list_of_files':   0,   # 1  0  0
# cp_files              1
# change_file_list      1
# def_makeDump(fn):     1
'extract_info_inside':  0,   # 0  1  0
'save_json':            0,   # 0  1  0
'combine_old_new':      0,   # 0  1  0
# cp_fileDict           0
# delList__             0
'facet_list':           1,   # 0  0  1
'save_json_for_web':    1,   # 0  0  1
# put_to_web            1
'':        0,   # 1  1
'':        0,   # 1  1
'':        0,   # 1  1
}

#if cr['']       == 1:
#if cr['']       == 1:
#if cr['']       == 1:
#if cr['']       == 1:

# 1 -- 
# 

if cr['import_']       == 1:
  # Python
  import os, sys, string, time, glob
  import re
  import traceback
  import pickle
  import json
  import yaml
  sys.path.append('%s/bin/py'%home)
  #import btang

  import tempfile

  # Numeric
  if 0:
    import Numeric as Num
    import RandomArray as RA
    import numpyIO
    import Num_btang as Num1

  # numpy
  import numpy as np
  import numpy_btang as np1

  if 0:
    #import scikits
    pass

  # netCDF
  if 0:
    import Scientific.IO.NetCDF as NC
    import NC_btang as NC1

    from netCDF4 import Dataset

    import Nio

  # from mpl_toolkits.basemap import NetCDFFile

    import pyferret as PF

if 1:
  sys.path.append('/home/svc/new_github/CMDA/JPL_CMDA/services/svc/svc/src/py')
  import  checkNc2
  reload(checkNc2)


#btang.tee(HOME + '/outputs/shtpy%s_%s'%(codeNum, time.strftime('%Y%m%d_%H%M%S')))

t00 = time.time()
print '======================'
print 'beginning: shtpy%s'%codeNum, time.ctime(time.time())
print '======================\n'

# readme__
'''
file affix:
0 -- the current calc
9 -- the previously saved one

files.js -- available nc files 
filesDict.js -- dict2, the good dict 
filesFailed.js -- failedData, the failed files 

fileList.js -- for web

'''

# parameters_
#mainDir = '/mnt/data_2016/cmip5'
#mainDir = '/mnt/data/data_2017/cmip5'
mainDir = '/mnt/data/data_clean/cmip5'
dataDir = mainDir

outDir = '/home/bytang'
#outDir = '/home/svc'

#nFile = 100
nFile = -1 # do all

readable = True
#readable = False

saveTo = '0'   # saved to a temp with affix '0'
#saveTo = '9'   # saved to a more permenant file

redoFailData = 0  # do whole data dir
#redoFailData = 1  # read from filesFailed9.js
redoFailData = 2  # not in the database, so including the failed ones and new ones
#redoFailData = 3  # from a list of files

dumpDir = '/mnt/cmda_mnt/dump_files'
tempfile.tempdir = dumpDir

'''
rsus_Amon_CMCC-CM_amip_r1i1p1_197901-198812.nc

'''

#argo/  bcc/  cccma/  cmcc/  cnrm/  csiro/  ecmwf/  fub-dwd/  gfdl/  giss/  iap/  inm/  ipsl/  miroc/  mpi/  mri/  nasa/  ncar/  ncc/  noaa/  ukmo/  uoe/  wcrp/  wrf/

'''
argo/
bcc/
cccma/
cmcc/
cnrm/
csiro/
ecmwf/
fli/
fub-dwd/
gfdl/
giss/
iap/
inm/
ipsl/
miroc/
mpi/
mri/
nasa/
ncar/
ncc/
noaa/
tkubar/
ukmo/
uoe/
wcrp/
wrf/
'''
providers = {
'noaa': 'NOAA',
'inm': 'INM',
'wcrp': 'WCRP',
'wrf': 'WRF',
'ecmwf': 'ECMWF',
'fub-dwd': 'FUB-DWD',
'bcc': 'BCC',
'nasa': 'NASA',
'uoe': 'UOE',
'ukmo': 'UKMO',
'iap': 'IAP',
'giss': 'GISS',
'mpi': 'MPI',
'argo': 'ARGO',
'ncc': 'NCC',
'cnrm': 'CNRM',
'ipsl': 'IPSL',
'cccma': 'CCCMA',
'miroc': 'MIROC',
'mri': 'MRI',
'ncar': 'NCAR',
'csiro': 'CSIRO',
'cmcc': 'CMCC',
'gfdl': 'GFDL',
}

providers2 = {}
for prov in providers.values():
  providers2[prov] = ['_' + prov.lower() + '-', '_' + prov.lower() + '_']

models = [
'ECMWF_interim',
'ERA_interim',
'GFDL-ESM2G',
'SSMI-MERIS',
'GFDL-CM3',
'GISS-E2-H',
'GISS-E2-R',
'FGOALS-s2',
'PSL-CM5A-LR',
'MIROC5',
'MIROC4h',
'MPI-ESM-LR',
'MRI-CGCM3',
'MODIS',
'TRMM',
'CESM1-CAM5',
'NorESM1-ME',
'NorESM1-M',
'NOAA',
'HadGEM2-A',
'HadGEM2-ES',
'ATSR',
'ISCCP',
'ARGO',
'CanESM2',
'CanAM4',
'CNRM-CM5',
'CSIRO-MK3-6-0',
'CMCC-CM',
'NCC-NORESM',
'bbc-csm1-1',
'inmcm4',
'CM3',
'CM5A-LR',
'MIROC5',
'HadGEM2-A',
'CM5',
'SSMI-MERIS',
'ESM-LR',
'CGCM3',
'GRACE',
'AMSRE',
'MLS',
'AIRS',
'GPCP-SG',
'CERES-SG',
'CERES-EBAF',
'QuikSCAT',
'AVISO',
'cloudSat-CALIPSO',
'MISR',
'2C-ICE',
'TES',
'CALIOP',
'CALIOP-GOCCP-v2.1',
'CESM1-CAM5',
'ARGO',
#
'wrf_st',
'wrf_ra-cam',
'wrf_pbl-ysu',
'wrf_pbl-nn2',
'wrf_mp-wdm',
'wrf_mp-mor',
'wrf_co-kf',
'wrf_co-tie',
'wrf_st',
#
'bcc-csm1-1',
'inmcm4',
]

models2 = {}
for mod1 in models:
  #models2[mod1] =  ['-' + mod1.lower() + '_', '_' + mod1.lower() + '_']
  models2[mod1] =  '_' + mod1.lower() + '_'

experiments = [
'historical',
'esmHistorical',
'historicalExt',
'amip',
'amip4XCO2',
'obs4MIPs',
]

experiments2 = {}
for exp1 in experiments:
  experiments2[exp1] =  '_' + exp1.lower() + '_' 

runs = [
'r1i1p1',
]

runs2 = {}
for rr in runs:
  runs2[rr] =  '_' + rr.lower() + '_' 

#os.chdir(mainDir)
# parameters__

fileList2 = None

if readable:
  indent1 = 2
  separators1 = (',', ':')
else:
  indent1 = None
  separators1 = None

if 'beenCalled' in dir():
  if beenCalled==1:
    print '=== using dataDir_z plotAffix_z =='
    time.sleep(2)
    dataDir = dataDir_z

if cr['command_arg']       == 1:
  if len(sys.argv)>1:
    argv = sys.argv
    year   = int(argv[1])
    month  = int(argv[2])
    day    = int(argv[3])

if cr['collect_fileList']       == 1:
  fileList = []
  exclude1 = ['no_good',]
  for t,d,f in os.walk(mainDir, followlinks=True):
    #print(t)
    #print(d)
    d[:] = [dd for dd in d if dd not in exclude1]
    for ff in f:
      temp1 = ff.rfind('.')
      if ff[temp1:] in ('.nc', '.cdf'):
        fileList.append(os.path.join(t,ff))
  
  print 'There are %d files'%len(fileList)
  for i in range(10):
    print fileList[i]

  if 1:
    fid = open('%s/fileList.txt'%outDir, 'w')
    fid.write( '\n'.join(fileList) ) 
    fid.close()

if cr['combine_break_files']       == 1:
  p1 = [ 
    [re.compile(r'(.*)_(\d+)-(\d+).nc'),  '%s_*-*.nc' ],  
    [re.compile(r'(.*)_(\d+)-(\d+).cdf'), '%s_*-*.cdf'], 
    [re.compile(r'(.*)_(\d+)_(\d+).nc'),  '%s_*_*.nc' ],  
    [re.compile(r'(.*)_(\d+)_(\d+).cdf'), '%s_*_*.cdf'], 
    [re.compile(r'(.*)_(\d+).nc'),        '%s_*.nc' ],  
    [re.compile(r'(.*)_(\d+).cdf'),       '%s_*.cdf'],   
    [re.compile(r'(.*)_(\d+).nc'),        '%s_*.nc'  ],   
    #[re.compile(r'(.*).cdf'),             '%s.cdf' ],   
  ]
  fileList2 = {}

  toLoopNext = fileList
  for pp in p1:
    toLoop = toLoopNext
    toLoopNext = []
    for f in toLoop:
      try:
        g1 = pp[0].search(f).groups()
      except:
        toLoopNext.append(f)
        continue

      try: 
        fileList2[ pp[1]%g1[0] ].append(f)
      except:
        fileList2[ pp[1]%g1[0] ] = [f,]

  nMF = len(fileList2) 

  # if the wildcard produces only 1 file, make it the filename itself
  nSF = 0
  for f in fileList2.keys():
    if len(fileList2[f])==1: 
      fileList2[ fileList2[f][0] ] = [ fileList2[f][0], ]
      del fileList2[f] 
      nSF += 1

  for f in toLoopNext:
    fileList2[f] = [f,]

  print '# of MF datasets: ', len(fileList2) - len(toLoopNext) - nSF
  print '# of SF datasets: ', len(toLoopNext) + nSF
  print '# of SF datasets (no fn date): ', len(toLoopNext)
  
  print 'before combining, there are %d datasets'%len(fileList)
  print 'after combining,  there are %d datasets'%len(fileList2)

  if 0: 
    keys1 = fileList2.keys()
    for i in range(20):
      print '\nkey: ', keys1[i]
      print fileList2[ keys1[i] ]

    for i in range(20):
      print '\nkey: ', keys1[-i-1]
      print fileList2[ keys1[-i-1] ]

if cr['save_list_of_files']       == 1:
  fid = open('%s/files%s.js'%(outDir, saveTo),'w')
  fid.write( json.dumps(fileList2, indent=indent1, separators=separators1))
  fid.close()

# cp_files
  '''
cd /home/bytang
mv files0.js files9.js

'''

# def_makeDump(fn):
def makeDump(fn, dumpCount):
  dumpCount[0] += 1

  fnDump0 = None
  try:
    if fn.find('*')>-1:
      fn2 = glob.glob(fn)
    else:
      fn2 = [fn,]

    count1 = 0
    for ff in fn2:
      count1 += 1

      # only do ncdump for the first 2
      if count1>2:
        break

      tempStr = tempfile.mktemp()
      tempStr = os.path.split(tempStr)[1]

      fnDump = '%s/dump_%s_%05d'%(dumpDir, tempStr,  dumpCount[0])

      temp1 = 'ncdump -c %s > %s'%(ff, fnDump)
      print(temp1)
      a = os.system(temp1)

      try:
        a2 = os.path.getsize(fnDump)
      except:
        a2 = 0

      if a==0 and a2>10:
        #print('dump:\n%s'%fnDump)
        print('%s'%fnDump)

        # if the first ncdump is good:
        if count1==1:
          fnDump0 = fnDump

  except:
    print(traceback.format_exc()) 
    return None

  return fnDump0


if cr['extract_info_inside']       == 1:
  from netCDF4 import MFDataset
  dict2 = {}
  failedData = {}
  count1 = 0
  dumpCount = [0,]

#redoFailData = 0  # do whole data dir
#redoFailData = 1  # read from filesFailed9.js
#redoFailData = 2  # not in the database, so including the failed ones and new ones
#redoFailData = 3  # from a list of files

  if redoFailData==0:
    if fileList2:
      keys1 = fileList2.keys()
    else:
      fn2 = '%s/files9.js'%outDir
      keys1 = yaml.safe_load( open(fn2).read() ).keys()

  elif redoFailData==1:
    fn2 = '%s/filesFailed9.js'%outDir
    failedData0 = yaml.safe_load( open(fn2).read() )
    keys1 = failedData9.keys()

  elif redoFailData==2:
    #fn2 = '%s/files9.js'%outDir
    #filesAll = yaml.safe_load( open(fn2).read() ).keys()

    if fileList2 or False:
      filesAll = fileList2.keys()
    else:
      fn2 = '%s/files9.js'%outDir
      filesAll = yaml.safe_load( open(fn2).read() ).keys()

    fn2 = '%s/filesDict9.js'%outDir
    filesGood = yaml.safe_load( open(fn2).read() ).keys()

    keys1 = [i for i in filesAll if i not in filesGood]

# change_file_list
  elif redoFailData==3:
    d9 = {  
'/mnt/data/data_clean/cmip5/others/sst.wkmean_*-*.nc':1,
'/mnt/data/data_clean/cmip5/users/tkubar/ecmwf_interim_uwind_profile_1pt5x1pt5_monthly_*.nc':1,
#'/mnt/data/data_clean/cmip5/wrf/co-kf/rlut_Amon_wrf_co-kf_200401-200412.nc':1,
#'/mnt/data/data_clean/cmip5/users/tkubar/output_monthly_misr_terra_l3_cloud_top_height_dist_apr2002_jan2019.nc':1,
}
    keys1 = d9.keys()

  if nFile==-1:
    keys2 = keys1
  else:
    keys2 = keys1[:nFile]

  collectOk1 = {}

  for k in keys2:
    count1 += 1
    print 'doing %d of %d: %s'%(count1, len(keys1), k)
    dump1 = makeDump(k, dumpCount)
    #f1 = fileList2[k]
    #f2 = ','.join(fileList2[k])
    dict1 = {'message': ''}
    try:
      ok1 = checkNc2.checkNc(k, dict1, overwrite=0, allowOverwrite=0)
      #print 'ok1: ', ok1
    except:
      ok1 = 99
      print(traceback.format_exc()) 
 
    if ok1==0:
      del dict1['message']
      del dict1['warning']
      del dict1['check']
      dict2[k] = dict1 
    else:
      failedData[k] = [ok1, dump1]

    print('ok1 = %d'%ok1)
    print('\n\n')
    collectOk1[k] = ok1

  count2 = {}
  for k in collectOk1.keys():
    try:
      count2[collectOk1[k]] +=1
    except:
      count2[collectOk1[k]] = 1

  print('How many ok1:')
  for ok2 in count2.keys():
    print('%2d  %5d'%(ok2,count2[ok2]))

  print('\n')
  print 'success: %6d / %d'%(len(dict2), len(keys2))
  print 'failed : %6d / %d'%(len(failedData), len(keys2))
  #nc = MFDataset(fileList2[k])
  #varListAll = nc.variables.keys()

if cr['save_json']       == 1:
  fid = open('%s/filesDict01.js'%outDir,'w')
  fid.write( json.dumps(dict2, indent=indent1, separators=separators1) )
  fid.close()

  fid = open('%s/filesFailed01.js'%outDir,'w')
  fid.write( json.dumps(failedData, indent=indent1, separators=separators1) )
  fid.close()

if cr['combine_old_new']       == 1:
  fn2 = '%s/filesDict9.js'%outDir
  if os.path.isfile(fn2):
    filesGoodOld = yaml.safe_load( open(fn2).read() )

    fn1 = '%s/filesDict01.js'%outDir
    filesGoodNew = yaml.safe_load( open(fn1).read() )

    kOld = filesGoodOld.keys()
    kNew = filesGoodNew.keys()

    for k in kNew:
      filesGoodOld[k] = filesGoodNew[k]

    print('-- Combining old and new:')
    print('Number of old:      %5d'%len(kOld))
    print('Number of new:      %5d'%len(kNew))
    print('Number of combined: %5d'%len(filesGoodOld))

    fid = open('%s/filesDict%s.js'%(outDir, saveTo),'w')
    fid.write( json.dumps(filesGoodOld, indent=indent1, separators=separators1) )
    fid.close()

  else:
    print('No old filesDict9.js')
    print('Number of new:      %5d'%len(kNew))

    temp1 = 'cp %s %s'%(fn1, fn2)
    print(temp1)
    os.system(temp1)

# cp_fileDict
  '''
cd /home/bytang
mv filesDict0.js filesDict9.js

'''

if cr['facet_list']       == 1:
  sourceL = {}
  providerL = {}
  modelL = {}
  runL = {}
  experimentL = {}

  varL = {}
  varLongL = {}

  fn2 = '%s/filesDict9.js'%outDir
  dict2 = yaml.safe_load( open(fn2).read() )

  dimLen = {}
  dataset2see = {}
  wrongDim = {}
  searchText = {}
# delList__ datasets to exclude  
  delList = [
'/mnt/data/data_clean/cmip5/tkubar/low-cloud-study/mse_ratio_lower_ECMWF-INTERIM_200207-200512.nc',
#"/mnt/data/data_clean/cmip5/nasa/gldas/ncep_gldas_mrso1_*-*.nc",
#"/mnt/data/data_clean/cmip5/nasa/gldas/ncep_gldas_mrso1_1981-1989.nc",
#"/mnt/data/data_clean/cmip5/nasa/gldas/ncep_gldas_mrso1_197901-201806.nc",
#"/mnt/data/data_clean/cmip5/nasa/gldas/ncep_gldas_mrso2_197901-201806.nc",
#"/mnt/data/data_clean/cmip5/nasa/gldas/ncep_gldas_mrso2_201807-201906.nc",
#"/mnt/data/data_clean/cmip5/nasa/gldas/ncep_gldas_mrso3_197901-201806.nc",
#"/mnt/data/data_clean/cmip5/nasa/gldas/ncep_gldas_mrso3_201807-201906.nc",
]

  root1 = '/mnt/data/data_clean/cmip5/'
  nRoot1 = len(root1)
  keys2 = dict2.keys()
  print('len(dict2), before')
  print(len(dict2))
  for k in keys2:

    # how many dim    
    dLen = len(dict2[k]['dimList'])
    try: 
      dimLen[dLen] += 1
    except:
      dimLen[dLen] = 1

    # see which datasets has 0,1 dim
    #if dLen in (0,1):
    if dLen in (0,):
      wrongDim[k] = dLen

      if 1:
        print('0 var. not using: %s'%k)
        del dict2[k]
        continue

    # a list of datasets to be excluded
    if 1:
      if k in delList:
        del dict2[k]
        print('==== not using: %s'%k)
        continue

    # search text
    text1 = '%s: v=%s v-long=%s t=%s'%(
         k[nRoot1:], 
         '_'.join(dict2[k]['varList']), 
         '__'.join(dict2[k]['varListLong']), 
         dict2[k]['title']
    )  
    searchText[text1] = k

    try: 
      sourceL[ dict2[k]['source']] += 1 
    except:
      sourceL[ dict2[k]['source']] = 1 

    try: 
      providerL[ dict2[k]['provider']] += 1 
    except:
      providerL[ dict2[k]['provider']] = 1 
 
    try: 
      modelL[ dict2[k]['model']] += 1 
    except:
      modelL[ dict2[k]['model']] = 1 
 
    try: 
      runL[ dict2[k]['run']] += 1 
    except:
      runL[ dict2[k]['run']] = 1 
 
    try: 
      experimentL[ dict2[k]['experiment']] += 1 
    except:
      experimentL[ dict2[k]['experiment']] = 1 
 
    for vv in dict2[k]['varList']:
      try: 
        varL[ vv ] += 1 
      except:
        varL[ vv ] = 1 
 
    for vv in dict2[k]['varListLong']:
      try: 
        varLongL[ vv ] += 1 
      except:
        varLongL[ vv ] = 1 
 
  print('len(dict2), before')
  print(len(dict2))

  print('dimLen:')
  for i in dimLen.keys():
    print('%d: %d'%(i,dimLen[i]))

  if 0:
    print('wrongDim:')
    for i in wrongDim.keys():
      print('"%s":%d,'%(i, wrongDim[i]))
  
  if 0:
    sourceL = sourceL.keys()
    try:
      del sourceL[sourceL.index("_")] 
    except: pass
    sourceL = sorted(sourceL, key=lambda s: s.lower())

  sourceL = []
  sourceL.insert(0, 'uploaded')
  sourceL.insert(0, 'staged')
  sourceL.insert(0, 'online')
  sourceL.insert(0, '_')

  providerL = providerL.keys()
  try:
    del providerL[providerL.index("_")] 
  except: pass
  providerL = sorted(providerL, key=lambda s: s.lower())
  providerL.insert(0, '_')

  modelL = modelL.keys()
  modelL = sorted(modelL, key=lambda s: s.lower())
  try:
    del modelL[modelL.index("_")] 
  except: pass
  modelL.insert(0, '_')

  experimentL = experimentL.keys()
  experimentL = sorted(experimentL, key=lambda s: s.lower())
  try:
    del experimentL[experimentL.index("_")] 
  except: pass
  experimentL.insert(0, '_')

  runL = runL.keys()
  try:
    del runL[runL.index("_")] 
  except: pass
  runL = sorted(runL, key=lambda s: s.lower())
  runL.insert(0, '_')

  varL = varL.keys()
  try:
    del varL[varL.index("_")] 
  except: pass
  varL = sorted(varL, key=lambda s: s.lower())

  varLongL = varLongL.keys()
  try:
    del varLongL[varLongL.index("_")] 
  except: pass
  varLongL = sorted(varLongL, key=lambda s: s.lower())
  varLongL.insert(0, '_')

if cr['save_json_for_web']       == 1:
  fid = open('%s/fileList0.js'%outDir,'w')
  fid.write( 'fileDict = \n' + json.dumps(dict2, indent=indent1, separators=separators1) + ';' )
  fid.write( 'searchText = \n' + json.dumps(searchText, indent=indent1, separators=separators1) + ';' )
  fid.write( '\nsourceL = \n' + json.dumps(sourceL, indent=indent1, separators=separators1)  + ';')
  fid.write( '\nproviderL = \n' + json.dumps(providerL, indent=indent1, separators=separators1)  + ';')
  fid.write( '\nmodelL = \n' + json.dumps(modelL, indent=indent1, separators=separators1)  + ';')
  fid.write( '\nexperimentL = \n' + json.dumps(experimentL, indent=indent1, separators=separators1)  + ';')
  fid.write( '\nrunL = \n' + json.dumps(runL, indent=indent1, separators=separators1)  + ';')
  fid.write( '\nvarL = \n' + json.dumps(varL, indent=indent1, separators=separators1)  + ';')
  fid.write( '\nvarLongL = \n' + json.dumps(varLongL, indent=indent1, separators=separators1)  + ';')
  fid.close()
     
# put_to_web
'''
#== on my laptop ubuntu 20.04:
# for old cmda server:
# do this as user svc:
outdir=/home/bytang
cd /home/svc/new_github/CMDA/JPL_CMDA/frontend/public/html/js2
rsync -av $outdir/fileList0.js fileList_cloud_full.js

# diff
outdir=/home/bytang
cd /home/svc/new_github/CMDA/JPL_CMDA/frontend/public/html/js2
vimdiff $outdir/fileList0.js fileList_cloud_full.js

# for new cmda server:
outdir=/home/bytang
rsync -av $outdir/fileList0.js fileList_cloud_full_readable.js

#rsync -av $outdir/fileList.js fileList_test_full.js

kk web7
cd js2
rsync8022 $cmdat:$outdir/fileList.js fileList_500_readable.js
#rsync8022 $cmdat:$outdir/fileList.js fileList_500_readable.js

'''
print 'time lapse: ', time.time() - t00 
print 'ending: ', time.ctime(time.time())
#btang.detee()
